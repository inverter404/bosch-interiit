{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BlackBox Evaluation",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inverter404/bosch-interiit/blob/main/BlackBox_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BlackBox Setting Evaluation loop notebook"
      ],
      "metadata": {
        "id": "PNv_BvciD_Q_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing libraries"
      ],
      "metadata": {
        "id": "ewko7cMM9OBF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GlujNkh6YF2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnigns('ignore')\n",
        "from skimage.transform import resize\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import Parameter\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import init\n",
        "\n",
        "import torch\n",
        "import json\n",
        "from torchvision.transforms import Compose, Lambda\n",
        "from torchvision.transforms._transforms_video import (\n",
        "    CenterCropVideo,\n",
        "    NormalizeVideo,\n",
        ")\n",
        "from pytorchvideo.data.encoded_video import EncodedVideo\n",
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    ShortSideScale,\n",
        "    UniformTemporalSubsample,\n",
        "    UniformCropVideo\n",
        ")\n",
        "from typing import Dict\n",
        "\n",
        "!pip -q install pytorch_ranger\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from pytorch_ranger import Ranger\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze | grep tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbrdbREFf7vd",
        "outputId": "742af076-13ba-4514-ba92-7b47a96a6c7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensorflow @ file:///tensorflow-2.8.0-cp37-cp37m-linux_x86_64.whl\n",
            "tensorflow-datasets==4.0.1\n",
            "tensorflow-estimator==2.8.0\n",
            "tensorflow-gcs-config==2.8.0\n",
            "tensorflow-hub==0.12.0\n",
            "tensorflow-io-gcs-filesystem==0.24.0\n",
            "tensorflow-metadata==1.7.0\n",
            "tensorflow-probability==0.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze | grep torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKlL5Gg9gFz_",
        "outputId": "c10ad32b-c82f-4233-c7e3-1663ee6d4201"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch @ https://download.pytorch.org/whl/cu111/torch-1.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl\n",
            "torchaudio @ https://download.pytorch.org/whl/cu111/torchaudio-0.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl\n",
            "torchsummary==1.5.1\n",
            "torchtext==0.11.0\n",
            "torchvision @ https://download.pytorch.org/whl/cu111/torchvision-0.11.1%2Bcu111-cp37-cp37m-linux_x86_64.whl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze | grep sklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfJYeaF_gFxV",
        "outputId": "cc374576-04d2-447b-e1ea-087d516549ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sklearn==0.0\n",
            "sklearn-pandas==1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze | grep scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgLy_yQpgFuq",
        "outputId": "79e3b147-212f-4e56-eeb5-66aec09c9458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scikit-learn==1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze | grep pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZYDHSExgFrt",
        "outputId": "c8e4173a-5d39-423e-a2e4-024f7ec4dcec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pandas==1.3.5\n",
            "pandas-datareader==0.9.0\n",
            "pandas-gbq==0.13.3\n",
            "pandas-profiling==1.4.1\n",
            "sklearn-pandas==1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze | grep cv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcHxLv05gFpu",
        "outputId": "c3a38b10-5cb7-4f28-8708-27521970d1e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cvxopt==1.2.7\n",
            "cvxpy==1.0.31\n",
            "opencv-contrib-python==4.1.2.30\n",
            "opencv-python==4.1.2.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze | grep opencv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxlVt22LgqD8",
        "outputId": "a5093e22-8f4d-4eb2-91db-2c7e72f4e39b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "opencv-contrib-python==4.1.2.30\n",
            "opencv-python==4.1.2.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze | grep numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZMuP86xgufZ",
        "outputId": "19e8a511-677b-40d1-e1fb-97cff0c0eec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numpy==1.21.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation loop"
      ],
      "metadata": {
        "id": "xve3Z5ZECHAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "side_size = 256\n",
        "mean = [0, 0, 0]\n",
        "std = [1, 1, 1]\n",
        "crop_size = 256\n",
        "num_frames = 32\n",
        "sampling_rate = 2\n",
        "frames_per_second = 30\n",
        "alpha = 4\n",
        "\n",
        "class PackPathway(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Transform for converting video frames as a list of tensors.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, frames: torch.Tensor):\n",
        "        fast_pathway = frames\n",
        "        # Perform temporal sampling from the fast pathway.\n",
        "        slow_pathway = torch.index_select(\n",
        "            frames,\n",
        "            1,\n",
        "            torch.linspace(\n",
        "                0, frames.shape[1] - 1, frames.shape[1] // alpha\n",
        "            ).long(),\n",
        "        )\n",
        "        frame_list = [slow_pathway, fast_pathway]\n",
        "        return frame_list\n",
        "\n",
        "\n",
        "def mp42tensor(video_path):\n",
        "  '''\n",
        "  Converts MP4 video to tensor format.\n",
        "  :param \n",
        "      video_path: string, video file name\n",
        "  :return \n",
        "      frames: numpy array\n",
        "  '''\n",
        "  videocap = cv2.VideoCapture(video_path)\n",
        "  success, image = videocap.read()\n",
        "  cnt = 0\n",
        "  frames = []\n",
        "\n",
        "  while success:\n",
        "      image = resize(image, (224, 400))\n",
        "      frames.append(image)\n",
        "      success, image = videocap.read()\n",
        "      cnt += 1\n",
        "  frames = np.array(frames)\n",
        "  return frames\n",
        "\n",
        "# Transformation function applied to input videos\n",
        "transform =  ApplyTransformToKey( # Applies transform to key of dictionary input\n",
        "    key=\"video\",\n",
        "    transform=Compose(\n",
        "        [\n",
        "            UniformTemporalSubsample(num_frames), # Uniformly subsamples indices from the temporal dimension of the video\n",
        "            #Lambda(lambda x: x/255.0),\n",
        "            NormalizeVideo(mean, std), # Normalization\n",
        "            ShortSideScale(\n",
        "                size=side_size\n",
        "            ),\n",
        "            #CenterCropVideo(crop_size),\n",
        "            PackPathway()\n",
        "        ]\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "R5TwwPjO6wtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(video_path, student, labels, transform=transform):\n",
        "  '''\n",
        "  Evaluation loop for calculating accuracy using test data\n",
        "  :param video_path: string, video file name in test dataset\n",
        "  :param student: Student/attacker model \n",
        "  :param labels: \n",
        "  :param transform: transformation to be applied on the video tensors\n",
        "  '''\n",
        "  vid_inp = torch.from_numpy(mp42tensor(video_path))\n",
        "  # print(vid_inp.unsqueeze(0).shape)\n",
        "  # pred_vic = api.movinet(vid_inp)\n",
        "  # pred_vic = torch.tensor(pred_vic.numpy()).to(device)\n",
        "\n",
        "  vid_inp = vid_inp.unsqueeze(0).to(device, dtype=torch.float32)\n",
        "  # print(vid_inp.shape)\n",
        "\n",
        "  pyt_in = vid_inp.permute(0, 4, 1, 2, 3).cpu()\n",
        "  pred_stud = []\n",
        "\n",
        "  for i in range(pyt_in.shape[0]):\n",
        "    video_data = transform({\"video\": pyt_in[i]})\n",
        "    inputs = video_data[\"video\"]\n",
        "    inputs = [inp.to(device)[None, ...] for inp in inputs]\n",
        "    pred_stud.append(student(inputs))\n",
        "\n",
        "  pred_stud = torch.cat(pred_stud, 0)\n",
        "  pred_stud = pred_stud.to(device)\n",
        "  # ind_vic = pred_vic.argmax(dim=1)\n",
        "  ind_vic = pred_stud.argmax(dim=1)\n",
        "\n",
        "  top5 = torch.topk(pred_stud.flatten(), 5).indices\n",
        "\n",
        "  if labels in top5:\n",
        "    return 1 \n",
        "  return 0"
      ],
      "metadata": {
        "id": "7SXr8nhi6_yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('/content/testwithlabels.csv') # Loading the test dataset\n",
        "print(len(test))\n",
        "test.head()"
      ],
      "metadata": {
        "id": "l1DXjUDs7G_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k600 = os.listdir('/content/drive/MyDrive/test') # Creating directory of the videos available in test dataset\n",
        "print(len(k600))"
      ],
      "metadata": {
        "id": "oajBULlO7HpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the length of k600 and test is different, we will find out the intersection of the video files from the both test and k600 which will be used for evaluation purposes"
      ],
      "metadata": {
        "id": "kfS6-v0eCUcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id_match = []\n",
        "count_ = 0\n",
        "\n",
        "for i in range(len(k600)):\n",
        "  if k600[i][:11] in test['youtube_id'].values: # Matching the first 11 characters of the video file name\n",
        "    count_ += 1\n",
        "    id_match.append(k600[i]) # Appending the names to a list\n",
        "print(count_)"
      ],
      "metadata": {
        "id": "rdiUjwyB7LrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_id_match = []\n",
        "for i in range(len(id_match)):\n",
        " new_id_match.append(id_match[i][:11]) # Appending the first 11 characters of the video file name"
      ],
      "metadata": {
        "id": "DY_SXn7B7Lnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic = {}\n",
        "for i in range(len(new_id_match)):\n",
        "  dic[new_id_match[i]] = id_match[i] # Creating a dictionary of key value pairs denoting first 11 characters and full video names respectively"
      ],
      "metadata": {
        "id": "4qbeQG3A7Llp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the key value pairs for all the matching/intersecting video file names, let's create a new dataset with only those videos"
      ],
      "metadata": {
        "id": "-8kUi_kSDZ3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = test[test['youtube_id'].isin(new_id_match)] # Creation of new dataframe for matching videos\n",
        "df['id_new'] = df['youtube_id']\n",
        "df.replace({'id_new':dic}, inplace=True) # remapping youtube ids with the complete video file name\n",
        "df.to_csv('/content/df600.csv', index=False)"
      ],
      "metadata": {
        "id": "j7yMKiLU7Lil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading the student model"
      ],
      "metadata": {
        "id": "uuiQDIyQDxRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "student_model = torch.load('/content/drive/MyDrive/interiit1503/movinet_weights/black_box_model.pth') ## Change according to path on machine"
      ],
      "metadata": {
        "id": "7PSUdtVv7Lgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Running the evaluation loop"
      ],
      "metadata": {
        "id": "PqkTlVfID1OW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root = '/content/drive/MyDrive/test/'\n",
        "correct_pred = 0\n",
        "\n",
        "for index, (id, label) in enumerate(tqdm(zip(df.id_new, df.id) , total=len(df))): # Iterating through the test dataframe\n",
        "  correct_pred += evaluate(root + id, student_model, label)\n",
        "  if index == 49:                \n",
        "    break\n",
        "print(f'Accuracy: {correct_pred/50}')"
      ],
      "metadata": {
        "id": "JKHrQ7LJ7iHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "a4nRrQ6q7iCA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}