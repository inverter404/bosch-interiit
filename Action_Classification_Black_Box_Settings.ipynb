{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inverter404/bosch-interiit/blob/main/Action_Classification_Black_Box_Settings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#REQUIREMENTS"
      ],
      "metadata": {
        "id": "pwl-LHu1uyTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##INSTALLING LIBRARIES"
      ],
      "metadata": {
        "id": "SW6mQj_au6Zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q timm\n",
        "!pip install -q einops\n",
        "!pip install -q -I mmcv==1.4.0\n",
        "!pip install -q pytorchvideo"
      ],
      "metadata": {
        "id": "xDExseEiu3xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##IMPORTING LIBRARIES"
      ],
      "metadata": {
        "id": "N9s-d6WsvBMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from typing import Dict\n",
        "from collections import OrderedDict\n",
        "\n",
        "from mmcv import Config, DictAction\n",
        "from mmaction.models import build_model\n",
        "from mmcv.runner import get_dist_info, init_dist, load_checkpoint\n",
        "from mmcv.parallel import MMDataParallel\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Parameter\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import init\n",
        "from torchvision.transforms import Compose, Lambda\n",
        "from torchvision.transforms._transforms_video import (\n",
        "    CenterCropVideo,\n",
        "    NormalizeVideo,\n",
        ")\n",
        "\n",
        "from pytorchvideo.data.encoded_video import EncodedVideo\n",
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    ShortSideScale,\n",
        "    UniformTemporalSubsample\n",
        "    # UniformCropVideo\n",
        ")"
      ],
      "metadata": {
        "id": "sUpitE23vccm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ssc8ZK1IviQq"
      },
      "source": [
        "# Swin Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BXPt_Vf3gj4"
      },
      "outputs": [],
      "source": [
        "!rm -rf Video-Swin-Transformer\n",
        "!git clone https://github.com/SwinTransformer/Video-Swin-Transformer.git\n",
        "%cd Video-Swin-Transformer\n",
        "!rm -rf /content/checkpoints\n",
        "!mkdir /content/checkpoints \n",
        "%cd /content/checkpoints\n",
        "!wget -q https://github.com/SwinTransformer/storage/releases/download/v1.0.4/swin_tiny_patch244_window877_kinetics400_1k.pth\n",
        "%cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFoqlKNKVz22",
        "outputId": "01018967-a117-46ff-b4c1-3bd817632f05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/checkpoints\n",
            "/content\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "config = '/content/Video-Swin-Transformer/configs/recognition/swin/swin_tiny_patch244_window877_kinetics400_1k.py'\n",
        "checkpoint = '/content/checkpoints/swin_tiny_patch244_window877_kinetics400_1k.pth'\n",
        "\n",
        "cfg = Config.fromfile(config)\n",
        "model = build_model(cfg.model, train_cfg=None, test_cfg=cfg.get('test_cfg'))\n",
        "\n",
        "\n",
        "\n",
        "class SwinT(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(SwinT, self).__init__()\n",
        "        self.backbone = model\n",
        "        self.cls_head = nn.Linear(768, 400)\n",
        "        self.dropout = nn.Dropout(p=0.5, inplace=False)\n",
        "        self.pool = nn.AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
        "\n",
        "    def forward(self,x):\n",
        "        feat = self.backbone(x)\n",
        "        feat = self.dropout(feat)\n",
        "        feat = self.pool(feat)\n",
        "        feat = feat.view(-1, 768)\n",
        "        return self.cls_head(feat)\n",
        "\n",
        "black_box_model = SwinT(model.backbone)\n",
        "\n",
        "new_state_dict = OrderedDict()\n",
        "checkpoint = torch.load(\"/content/checkpoints/swin_tiny_patch244_window877_kinetics400_1k.pth\")\n",
        "for k, v in checkpoint['state_dict'].items():\n",
        "    if 'backbone' in k:\n",
        "        name = k\n",
        "        new_state_dict[name] = v \n",
        "\n",
        "new_state_dict[\"cls_head.weight\"] = checkpoint[\"state_dict\"][\"cls_head.fc_cls.weight\"]\n",
        "new_state_dict[\"cls_head.bias\"] = checkpoint[\"state_dict\"][\"cls_head.fc_cls.bias\"]\n",
        "\n",
        "black_box_model.load_state_dict(new_state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6SVkjAdPxivN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_AjLez1osIB"
      },
      "source": [
        "# Generator \n",
        "(DVD-GAN: [Source](https://github.com/Harrypotterrrr/DVD-GAN))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTBnK2lNxi8F"
      },
      "source": [
        "### Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ma7voo_Oxi8F"
      },
      "outputs": [],
      "source": [
        "def l2normalize(v, eps=1e-12):\n",
        "    return v / (v.norm() + eps)\n",
        "\n",
        "class SpectralNorm(nn.Module):\n",
        "    def __init__(self, module, name='weight', power_iterations=1):\n",
        "        super(SpectralNorm, self).__init__()\n",
        "        self.module = module\n",
        "        self.name = name\n",
        "        self.power_iterations = power_iterations\n",
        "        if not self._made_params():\n",
        "            self._make_params()\n",
        "\n",
        "    def _update_u_v(self):\n",
        "        u = getattr(self.module, self.name + \"_u\")\n",
        "        v = getattr(self.module, self.name + \"_v\")\n",
        "        w = getattr(self.module, self.name + \"_bar\")\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        for _ in range(self.power_iterations):\n",
        "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
        "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
        "\n",
        "        sigma = u.dot(w.view(height, -1).mv(v))\n",
        "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
        "\n",
        "    def _made_params(self):\n",
        "        try:\n",
        "            u = getattr(self.module, self.name + \"_u\")\n",
        "            v = getattr(self.module, self.name + \"_v\")\n",
        "            w = getattr(self.module, self.name + \"_bar\")\n",
        "            return True\n",
        "        except AttributeError:\n",
        "            return False\n",
        "\n",
        "\n",
        "    def _make_params(self):\n",
        "        w = getattr(self.module, self.name)\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        width = w.view(height, -1).data.shape[1]\n",
        "\n",
        "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
        "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
        "        u.data = l2normalize(u.data)\n",
        "        v.data = l2normalize(v.data)\n",
        "        w_bar = Parameter(w.data)\n",
        "\n",
        "        del self.module._parameters[self.name]\n",
        "\n",
        "        self.module.register_parameter(self.name + \"_u\", u)\n",
        "        self.module.register_parameter(self.name + \"_v\", v)\n",
        "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
        "\n",
        "\n",
        "    def forward(self, *args):\n",
        "        self._update_u_v()\n",
        "        return self.module.forward(*args)\n",
        "\n",
        "class ConditionalNorm(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channel, n_condition=96):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channel = in_channel\n",
        "        self.bn = nn.BatchNorm2d(self.in_channel, affine=False)\n",
        "\n",
        "        self.embed = nn.Linear(n_condition, self.in_channel * 2)\n",
        "        self.embed.weight.data[:, :self.in_channel].normal_(1, 0.02)\n",
        "        self.embed.weight.data[:, self.in_channel:].zero_()\n",
        "\n",
        "    def forward(self, x, class_id):\n",
        "        out = self.bn(x)\n",
        "        embed = self.embed(class_id)\n",
        "        gamma, beta = embed.chunk(2, 1)\n",
        "        gamma = gamma.view(-1, self.in_channel, 1, 1)\n",
        "        beta = beta.view(-1, self.in_channel, 1, 1)\n",
        "        out = gamma * out + beta\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4090ntpxi8F"
      },
      "source": [
        "## GResBlock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-GIT2Jkxi8F"
      },
      "outputs": [],
      "source": [
        "class GResBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channel, out_channel, kernel_size=None,\n",
        "                 padding=1, stride=1, n_class=96, bn=True,\n",
        "                 activation=F.relu, upsample_factor=2, downsample_factor=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.upsample_factor = upsample_factor if downsample_factor is 1 else 1\n",
        "        self.downsample_factor = downsample_factor\n",
        "        self.activation = activation\n",
        "        self.bn = bn if downsample_factor is 1 else False\n",
        "\n",
        "        if kernel_size is None:\n",
        "            kernel_size = [3, 3]\n",
        "\n",
        "        self.conv0 = SpectralNorm(nn.Conv2d(in_channel, out_channel,\n",
        "                                             kernel_size, stride, padding,\n",
        "                                             bias=True if bn else True))\n",
        "        self.conv1 = SpectralNorm(nn.Conv2d(out_channel, out_channel,\n",
        "                                             kernel_size, stride, padding,\n",
        "                                             bias=True if bn else True))\n",
        "\n",
        "        self.skip_proj = True\n",
        "        self.conv_sc = SpectralNorm(nn.Conv2d(in_channel, out_channel, 1, 1, 0))\n",
        "\n",
        "\n",
        "\n",
        "        if bn:\n",
        "            self.CBNorm1 = ConditionalNorm(in_channel, n_class) # TODO 2 x noise.size[1]\n",
        "            self.CBNorm2 = ConditionalNorm(out_channel, n_class)\n",
        "\n",
        "    def forward(self, x, condition=None):\n",
        "\n",
        "        # The time dimension is combined with the batch dimension here, so each frame proceeds\n",
        "        # through the blocks independently\n",
        "        BT, C, W, H = x.size()\n",
        "        out = x\n",
        "\n",
        "        if self.bn:\n",
        "            out = self.CBNorm1(out, condition)\n",
        "\n",
        "        out = self.activation(out)\n",
        "\n",
        "        if self.upsample_factor != 1:\n",
        "            out = F.interpolate(out, scale_factor=self.upsample_factor)\n",
        "\n",
        "        out = self.conv0(out)\n",
        "\n",
        "        if self.bn:\n",
        "            out = out.view(BT, -1, W * self.upsample_factor, H * self.upsample_factor)\n",
        "            out = self.CBNorm2(out, condition)\n",
        "\n",
        "        out = self.activation(out)\n",
        "        out = self.conv1(out)\n",
        "\n",
        "        if self.downsample_factor != 1:\n",
        "            out = F.avg_pool2d(out, self.downsample_factor)\n",
        "\n",
        "        if self.skip_proj:\n",
        "            skip = x\n",
        "            if self.upsample_factor != 1:\n",
        "                skip = F.interpolate(skip, scale_factor=self.upsample_factor)\n",
        "            skip = self.conv_sc(skip)\n",
        "            if self.downsample_factor != 1:\n",
        "                skip = F.avg_pool2d(skip, self.downsample_factor)\n",
        "        else:\n",
        "            skip = x\n",
        "\n",
        "        y = out + skip\n",
        "        y = y.view(\n",
        "            BT, -1,\n",
        "            W * self.upsample_factor // self.downsample_factor,\n",
        "            H * self.upsample_factor // self.downsample_factor\n",
        "        )\n",
        "\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWijIW_gxi8G"
      },
      "source": [
        "## ConvGRUCell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TszmRjt3xi8G"
      },
      "outputs": [],
      "source": [
        "class ConvGRUCell(nn.Module):\n",
        "    \"\"\"\n",
        "    Generate a convolutional GRU cell\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, kernel_size, activation=torch.sigmoid):\n",
        "\n",
        "        super().__init__()\n",
        "        padding = kernel_size // 2\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.reset_gate = nn.Conv2d(input_size + hidden_size, hidden_size, kernel_size, padding=padding)\n",
        "        self.update_gate = nn.Conv2d(input_size + hidden_size, hidden_size, kernel_size, padding=padding)\n",
        "        self.out_gate = nn.Conv2d(input_size + hidden_size, hidden_size, kernel_size, padding=padding)\n",
        "        self.activation = activation\n",
        "\n",
        "        init.orthogonal_(self.reset_gate.weight)\n",
        "        init.orthogonal_(self.update_gate.weight)\n",
        "        init.orthogonal_(self.out_gate.weight)\n",
        "        init.constant_(self.reset_gate.bias, 0.)\n",
        "        init.constant_(self.update_gate.bias, 0.)\n",
        "        init.constant_(self.out_gate.bias, 0.)\n",
        "\n",
        "\n",
        "    def forward(self, x, prev_state=None):\n",
        "\n",
        "        if prev_state is None:\n",
        "\n",
        "            # get batch and spatial sizes\n",
        "            batch_size = x.data.size()[0]\n",
        "            spatial_size = x.data.size()[2:]\n",
        "\n",
        "            # generate empty prev_state, if None is provided\n",
        "            state_size = [batch_size, self.hidden_size] + list(spatial_size)\n",
        "            # prev_state = torch.zeros(state_size)\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                prev_state = torch.zeros(state_size).cuda()\n",
        "            else:\n",
        "                prev_state = torch.zeros(state_size)\n",
        "\n",
        "        # data size is [batch, channel, height, width]\n",
        "        stacked_inputs = torch.cat([x, prev_state], dim=1)\n",
        "\n",
        "        update = self.activation(self.update_gate(stacked_inputs))\n",
        "        reset = self.activation(self.reset_gate(stacked_inputs))\n",
        "        out_inputs = torch.tanh(self.out_gate(torch.cat([x, prev_state * reset], dim=1)))\n",
        "        new_state = prev_state * (1 - update) + out_inputs * update\n",
        "\n",
        "        return new_state\n",
        "\n",
        "\n",
        "class ConvGRU(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_sizes, kernel_sizes, n_layers):\n",
        "        \"\"\"\n",
        "        Generates a multi-layer convolutional GRU.\n",
        "\n",
        "        Parameters\n",
        "        -------\n",
        "        input_size : integer. depth dimension of input tensors.\n",
        "        hidden_sizes : integer or list. depth dimensions of hidden state.\n",
        "                      if integer, the same hidden size is used for all cells.\n",
        "        kernel_sizes : integer or list. sizes of Conv2d gate kernels.\n",
        "                      if integer, the same kernel size is used for all cells.\n",
        "        n_layers : integer. number of chained `ConvGRUCell`.\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "\n",
        "        if type(hidden_sizes) != list:\n",
        "            self.hidden_sizes = [hidden_sizes]*n_layers\n",
        "        else:\n",
        "            assert len(hidden_sizes) == n_layers, '`hidden_sizes` must have the same length as n_layers'\n",
        "            self.hidden_sizes = hidden_sizes\n",
        "        if type(kernel_sizes) != list:\n",
        "            self.kernel_sizes = [kernel_sizes]*n_layers\n",
        "        else:\n",
        "            assert len(kernel_sizes) == n_layers, '`kernel_sizes` must have the same length as n_layers'\n",
        "            self.kernel_sizes = kernel_sizes\n",
        "\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        cells = nn.ModuleList()\n",
        "\n",
        "        for i in range(self.n_layers):\n",
        "            if i == 0:\n",
        "                input_dim = self.input_size\n",
        "            else:\n",
        "                input_dim = self.hidden_sizes[i-1]\n",
        "\n",
        "            cell = ConvGRUCell(input_dim, self.hidden_sizes[i], self.kernel_sizes[i])\n",
        "            cells.append(cell)\n",
        "\n",
        "        self.cells = cells\n",
        "\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        '''\n",
        "        Parameters\n",
        "        -------\n",
        "        x : 4D input tensor. (batch, channels, height, width).\n",
        "        hidden : list of 4D hidden state representations. (batch, channels, height, width).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        upd_hidden : 5D hidden representation. (layer, batch, channels, height, width).\n",
        "        '''\n",
        "\n",
        "        input_ = x\n",
        "        output = []\n",
        "\n",
        "        if hidden is None:\n",
        "            hidden = [None] * self.n_layers\n",
        "\n",
        "        for i in range(self.n_layers):\n",
        "\n",
        "            cell = self.cells[i]\n",
        "            cell_hidden = hidden[i]\n",
        "\n",
        "            # pass through layer\n",
        "            upd_cell_hidden = cell(input_, cell_hidden) # TODO comment\n",
        "            output.append(upd_cell_hidden)\n",
        "            # update input_ to the last updated hidden layer for next pass\n",
        "            input_ = upd_cell_hidden\n",
        "\n",
        "        # retain tensors in list to allow different hidden sizes\n",
        "        return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzRfyVMbxi8H"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwRw2capxi8H"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim=120, latent_dim=4, n_class=4, ch=32, n_frames=48, hierar_flag=False):\n",
        "        '''\n",
        "        Parameters\n",
        "        -------\n",
        "        in_dim : dimension of the input vector\n",
        "        latent_dim : no of classes to be considered\n",
        "        ch : produced video channels\n",
        "        n_frames : no of frames for produced video\n",
        "        '''\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_dim = in_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.n_class = n_class\n",
        "        self.ch = ch\n",
        "        self.hierar_flag = hierar_flag\n",
        "        self.n_frames = n_frames\n",
        "\n",
        "        self.embedding = nn.Embedding(n_class, in_dim)\n",
        "\n",
        "        self.affine_transfrom = nn.Linear(in_dim * 2, latent_dim * latent_dim * 8 * ch)\n",
        "\n",
        "        self.conv = nn.ModuleList([\n",
        "            ConvGRU(8 * ch, hidden_sizes=[8 * ch, 16 * ch, 8 * ch], kernel_sizes=[3, 5, 3], n_layers=3),\n",
        "            # ConvGRU(8 * ch, hidden_sizes=[8 * ch, 8 * ch], kernel_sizes=[3, 3], n_layers=2),\n",
        "            GResBlock(8 * ch, 8 * ch, n_class=in_dim * 2, upsample_factor=1),\n",
        "            GResBlock(8 * ch, 8 * ch, n_class=in_dim * 2),\n",
        "            ConvGRU(8 * ch, hidden_sizes=[8 * ch, 16 * ch, 8 * ch], kernel_sizes=[3, 5, 3], n_layers=3),\n",
        "            # ConvGRU(8 * ch, hidden_sizes=[8 * ch, 8 * ch], kernel_sizes=[3, 3], n_layers=2),\n",
        "            GResBlock(8 * ch, 8 * ch, n_class=in_dim * 2, upsample_factor=1),\n",
        "            GResBlock(8 * ch, 8 * ch, n_class=in_dim * 2),\n",
        "            ConvGRU(8 * ch, hidden_sizes=[8 * ch, 16 * ch, 8 * ch], kernel_sizes=[3, 5, 3], n_layers=3),\n",
        "            # ConvGRU(8 * ch, hidden_sizes=[8 * ch, 8 * ch], kernel_sizes=[3, 3], n_layers=2),\n",
        "            GResBlock(8 * ch, 8 * ch, n_class=in_dim * 2, upsample_factor=1),\n",
        "            GResBlock(8 * ch, 4 * ch, n_class=in_dim * 2),\n",
        "            ConvGRU(4 * ch, hidden_sizes=[4 * ch, 8 * ch, 4 * ch], kernel_sizes=[3, 5, 5], n_layers=3),\n",
        "            # ConvGRU(4 * ch, hidden_sizes=[4 * ch, 4 * ch], kernel_sizes=[3, 5], n_layers=2),\n",
        "            GResBlock(4 * ch, 4 * ch, n_class=in_dim * 2, upsample_factor=1),\n",
        "            GResBlock(4 * ch, 2 * ch, n_class=in_dim * 2)\n",
        "        ])\n",
        "\n",
        "        # TODO impl ScaledCrossReplicaBatchNorm\n",
        "        # self.ScaledCrossReplicaBN = ScaledCrossReplicaBatchNorm2d(1 * chn)\n",
        "\n",
        "        self.colorize = SpectralNorm(nn.Conv2d(2 * ch, 3, kernel_size=(3, 3), padding=1))\n",
        "\n",
        "\n",
        "    def forward(self, x, class_id):\n",
        "\n",
        "        '''\n",
        "        Parameters\n",
        "        -------\n",
        "        x : input vector\n",
        "        class_id : video class to produced\n",
        "        \n",
        "        Return\n",
        "        -------\n",
        "        y : video tensor\n",
        "        '''\n",
        "        if self.hierar_flag is True:\n",
        "            noise_emb = torch.split(x, self.in_dim, dim=1)\n",
        "        else:\n",
        "            noise_emb = x\n",
        "\n",
        "        class_emb = self.embedding(class_id)\n",
        "\n",
        "        if self.hierar_flag is True:\n",
        "            y = self.affine_transfrom(torch.cat((noise_emb[0], class_emb), dim=1)) # B x (2 x ld x ch)\n",
        "        else:\n",
        "            y = self.affine_transfrom(torch.cat((noise_emb, class_emb), dim=1)) # B x (2 x ld x ch)\n",
        "\n",
        "        y = y.view(-1, 8 * self.ch, self.latent_dim, self.latent_dim) # B x ch x ld x ld\n",
        "\n",
        "        for k, conv in enumerate(self.conv):\n",
        "            if isinstance(conv, ConvGRU):\n",
        "\n",
        "                if k > 0:\n",
        "                    _, C, W, H = y.size()\n",
        "                    y = y.view(-1, self.n_frames, C, W, H).contiguous()\n",
        "\n",
        "                frame_list = []\n",
        "                for i in range(self.n_frames):\n",
        "                    if k == 0:\n",
        "                        if i == 0:\n",
        "                            frame_list.append(conv(y))  # T x [B x ch x ld x ld]\n",
        "                        else:\n",
        "                            frame_list.append(conv(y, frame_list[i - 1]))\n",
        "                    else:\n",
        "                        if i == 0:\n",
        "                            frame_list.append(conv(y[:,0,:,:,:].squeeze(1)))  # T x [B x ch x ld x ld]\n",
        "                        else:\n",
        "                            frame_list.append(conv(y[:,i,:,:,:].squeeze(1), frame_list[i - 1]))\n",
        "                frame_hidden_list = []\n",
        "                for i in frame_list:\n",
        "                    frame_hidden_list.append(i[-1].unsqueeze(0))\n",
        "                y = torch.cat(frame_hidden_list, dim=0) # T x B x ch x ld x ld\n",
        "\n",
        "                y = y.permute(1, 0, 2, 3, 4).contiguous() # B x T x ch x ld x ld\n",
        "                # print(y.size())\n",
        "                B, T, C, W, H = y.size()\n",
        "                y = y.view(-1, C, W, H)\n",
        "\n",
        "            elif isinstance(conv, GResBlock):\n",
        "                condition = torch.cat([noise_emb, class_emb], dim=1)\n",
        "                condition = condition.repeat(self.n_frames,1)\n",
        "                y = conv(y, condition) # BT, C, W, H\n",
        "\n",
        "        y = F.relu(y)\n",
        "        y = self.colorize(y)\n",
        "        y = torch.tanh(y)\n",
        "\n",
        "        BT, C, W, H = y.size()\n",
        "        y = y.view(-1, self.n_frames, C, W, H) # B, T, C, W, H\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjHOFH59xi8I"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvC4TMGnsJS3"
      },
      "source": [
        "# SlowFast Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATxPFrULsNL7"
      },
      "source": [
        "### Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IObwV6awswms"
      },
      "outputs": [],
      "source": [
        "##########################\n",
        "##  SlowFast transform  ##\n",
        "##########################\n",
        "\n",
        "side_size = 256\n",
        "mean = [0, 0, 0]\n",
        "std = [1, 1, 1]\n",
        "crop_size = 256\n",
        "num_frames = 32\n",
        "sampling_rate = 2\n",
        "frames_per_second = 30\n",
        "alpha = 4\n",
        "\n",
        "class PackPathway(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Transform for converting video frames as a list of tensors.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, frames: torch.Tensor):\n",
        "        fast_pathway = frames\n",
        "        # Perform temporal sampling from the fast pathway.\n",
        "        slow_pathway = torch.index_select(\n",
        "            frames,\n",
        "            1,\n",
        "            torch.linspace(\n",
        "                0, frames.shape[1] - 1, frames.shape[1] // alpha\n",
        "            ).long(),\n",
        "        )\n",
        "        frame_list = [slow_pathway, fast_pathway]\n",
        "        return frame_list\n",
        "  \n",
        "transform =  ApplyTransformToKey(\n",
        "    key=\"video\",\n",
        "    transform=Compose(\n",
        "        [\n",
        "            UniformTemporalSubsample(num_frames),\n",
        "            NormalizeVideo(mean, std),\n",
        "            ShortSideScale(\n",
        "                size=side_size\n",
        "            ),\n",
        "            PackPathway()\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "# The duration of the input clip is also specific to the model.\n",
        "clip_duration = (num_frames * sampling_rate)/frames_per_second"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm24hHL1xuBL"
      },
      "source": [
        "## Training Loop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xomtLIC7Aa0K"
      },
      "source": [
        "generator's output : [batch, 10, 3, 64, 64]\n",
        "\n",
        "tensorflow_model_input : [batch, 8, 224, 224, 3]\n",
        "\n",
        "pytorch_model_input : list of [batch, 3, 8, 256, 256]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/test /content/"
      ],
      "metadata": {
        "id": "lXU90uacVAzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDo3uYo03U2u"
      },
      "outputs": [],
      "source": [
        "!pip install -q pytorch_ranger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8x6IitvgDgd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_ranger import Ranger\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVYO52V0A5Cp"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\n",
        "# torch.manual_seed(1)\n",
        "# torch.cuda.manual_seed(1)\n",
        "\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUJgqH29xoD4",
        "outputId": "ade55ee2-af1c-4241-f31f-56f7fb425dc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_state(wt_path):\n",
        "  checkpoint = torch.load(wt_path)\n",
        "  generator.load_state_dict(checkpoint['generator_state'])\n",
        "  student.load_state_dict(checkpoint['student_state'])\n",
        "  gen_opt.load_state_dict(checkpoint['gen_optimizer'])\n",
        "  stud_opt.load_state_dict(checkpoint['stud_optimizer'])\n",
        "  return checkpoint['datapoints']"
      ],
      "metadata": {
        "id": "IrSygBVQwCJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKQgtmdGvene"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "black_box_model = black_box_model.to(device)\n",
        "num_epochs = 20\n",
        "gen_iter = 1\n",
        "stud_iter = 3\n",
        "n_datapoints = 100\n",
        "\n",
        "batch_size = 4\n",
        "in_dim = 120\n",
        "n_class = 400\n",
        "n_frames = 5\n",
        "\n",
        "Loss = nn.L1Loss()\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "#Initializing Generator\n",
        "class_label = torch.randint(low=0, high=399, size=(batch_size,)).to(device)\n",
        "generator = Generator(in_dim, n_class=n_class, ch=3, n_frames=n_frames).to(device)\n",
        "\n",
        "model_name = \"slowfast_r50\"\n",
        "student = torch.hub.load(\"facebookresearch/pytorchvideo\", model=model_name, pretrained=True)\n",
        "# For kinetics 400 \n",
        "student.blocks[6].proj = nn.Linear(in_features=2304, out_features=400, bias=True)\n",
        "student = student.to(device)\n",
        "black_box_model.eval()\n",
        "for param in generator.parameters():\n",
        "    param.requires_grad=True  \n",
        "\n",
        "for param in student.parameters():\n",
        "    param.requires_grad=True\n",
        "\n",
        "for param in black_box_model.parameters():\n",
        "    param.requires_grad=False\n",
        "\n",
        "gen_opt = Ranger(generator.parameters(), lr=5e-4, weight_decay=0.0001)\n",
        "\n",
        "\n",
        "stud_opt = Ranger(student.parameters(), lr=8e-5, weight_decay=0.0001)\n",
        "\n",
        "gen_loss_per_dp = []\n",
        "stud_loss_per_dp = []\n",
        "\n",
        "# torch.autograd.detect_anomaly()\n",
        "dp_done = load_state('/content/drive/MyDrive/model_dp56.pth')\n",
        "# dp_done = 0\n",
        "tk0 = tqdm(range(n_datapoints), desc='Datapoints')\n",
        "for datapoints in tk0:\n",
        "    if(datapoints < dp_done):\n",
        "      continue\n",
        "    print('x'*80)\n",
        "    print('x'*80)\n",
        "    batch = torch.randn(batch_size, in_dim).to(device) #fix this\n",
        "    gen_epoch_loss = []\n",
        "    stud_epoch_loss = []\n",
        "    tk1 = tqdm(range(num_epochs), desc='Epochs')\n",
        "    for epoch in tk1:\n",
        "      print(\"=\"*80)\n",
        "      print(f'Starting Epoch: {epoch+1} / {num_epochs}')\n",
        "      print(\"=\"*80)\n",
        "\n",
        "      student.eval()\n",
        "      generator.train()\n",
        "      gen_loss_arr = []\n",
        "      tk2 = tqdm(range(gen_iter), desc='Generator training...')\n",
        "      for _ in tk2:\n",
        "        y = generator(batch, class_label)\n",
        "        ## Prediction API Output ([batch_size, 400])\n",
        "        with torch.no_grad():\n",
        "          pred_vic = black_box_model(y.permute(0, 2, 1, 3, 4)).to(device)\n",
        "        pyt_in = y.permute(0, 2, 1, 3, 4).cpu()\n",
        "        pred_att = []\n",
        "        for i in range(pyt_in.shape[0]):\n",
        "            video_data = transform({\"video\": pyt_in[i]})\n",
        "            inputs = video_data[\"video\"]\n",
        "            inputs = [inp.to(device)[None, ...] for inp in inputs]\n",
        "            pred_att.append(student(inputs))\n",
        "        pred_att = torch.cat(pred_att, 0)\n",
        "        pred_att = pred_att.to(device)\n",
        "\n",
        "        gen_loss = (100-Loss(pred_att, pred_vic)) / 100\n",
        "        \n",
        "        scaler.scale(gen_loss).backward()\n",
        "        nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
        "        scaler.step(gen_opt)\n",
        "        scaler.update()\n",
        "  \n",
        "        torch.cuda.empty_cache()\n",
        "        gen_loss_arr.append(gen_loss.detach().cpu().numpy())\n",
        "        tk2.set_postfix(loss = gen_loss.detach().cpu().numpy())\n",
        "\n",
        "      gen_epoch_loss.append(np.mean(gen_loss_arr))\n",
        "  \n",
        "      student.train()\n",
        "      generator.eval()\n",
        "\n",
        "      stud_loss_arr = []\n",
        "      tk3 = tqdm(range(stud_iter), desc='Student training...')\n",
        "      for _ in tk3:\n",
        "        y = generator(batch, class_label)\n",
        "        ## Prediction API Output ([batch_size, 400])\n",
        "        with torch.no_grad():\n",
        "          pred_vic = black_box_model(y.permute(0, 2, 1, 3, 4)).to(device)\n",
        "        pyt_in = y.permute(0, 2, 1, 3, 4).cpu()\n",
        "        pred_att = []\n",
        "        for i in range(pyt_in.shape[0]):\n",
        "            video_data = transform({\"video\": pyt_in[i]})\n",
        "            inputs = video_data[\"video\"]\n",
        "            inputs = [inp.to(device)[None, ...] for inp in inputs]\n",
        "            pred_att.append(student(inputs))\n",
        "        pred_att = torch.cat(pred_att, 0)\n",
        "        pred_att = pred_att.to(device)\n",
        "\n",
        "        stud_loss = Loss(pred_att, pred_vic) \n",
        "        \n",
        "        scaler.scale(stud_loss).backward()\n",
        "        nn.utils.clip_grad_norm_(student.parameters(), max_norm=2.0)\n",
        "        scaler.step(stud_opt)#.step()  \n",
        "        scaler.update()\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        stud_loss_arr.append(stud_loss.detach().cpu().numpy())\n",
        "        tk3.set_postfix(loss = stud_loss.detach().cpu().numpy())\n",
        "      stud_epoch_loss.append(np.mean(stud_loss_arr))\n",
        "      torch.cuda.empty_cache()\n",
        "      \n",
        "      print(f'Student Loss: {stud_epoch_loss[-1]} \\t Generator Loss: {gen_epoch_loss[-1]}')\n",
        "    gen_loss_per_dp.append(np.mean(gen_epoch_loss))\n",
        "    stud_loss_per_dp.append(np.mean(stud_epoch_loss))\n",
        "    print(\"x\"*80)\n",
        "    print(\"Finished another datapoint iteration....Loading metrics....\")\n",
        "    print(f'Student Loss: {stud_loss_per_dp[-1]} \\t Generator Loss: {gen_loss_per_dp[-1]}')\n",
        "    print(\"x\"*80)\n",
        "    plt.plot(stud_epoch_loss, label=\"Student loss\")\n",
        "    plt.plot(gen_epoch_loss, label=\"Generator loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    save_state = {'datapoints':datapoints, 'student_state':student.state_dict(),\n",
        "             'generator_state':generator.state_dict(), 'stud_optimizer':stud_opt.state_dict(),\n",
        "             'gen_optimizer':gen_opt.state_dict()}\n",
        "    print(\"Saving model...\")\n",
        "    torch.save(save_state, f'/content/drive/MyDrive/SwinT Weights/model_dp{datapoints}.pth')\n",
        "plt.plot(stud_loss_per_dp, label=\"Student loss\")\n",
        "plt.plot(gen_loss_per_dp, label=\"Generator loss\")\n",
        "plt.xlabel(\"DataPoints\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Action Classification Black Box Settings.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}